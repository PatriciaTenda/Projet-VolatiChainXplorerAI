{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3f3ca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import uuid\n",
    "\n",
    "from IPython.display import display_javascript, display_html, display\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, date, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4d31139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.11\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d275c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/JustAnotherArchivist/snscrape.git"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/JustAnotherArchivist/snscrape.git 'C:\\Users\\PATRICIA\\AppData\\Local\\Temp\\pip-req-build-_v702nu4'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Cloning https://github.com/JustAnotherArchivist/snscrape.git to c:\\users\\patricia\\appdata\\local\\temp\\pip-req-build-_v702nu4\n",
      "  Resolved https://github.com/JustAnotherArchivist/snscrape.git to commit 614d4c2029a62d348ca56598f87c425966aaec66\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\patricia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from snscrape==0.7.0.20230622) (2.32.3)\n",
      "Requirement already satisfied: lxml in c:\\users\\patricia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from snscrape==0.7.0.20230622) (5.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\patricia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from snscrape==0.7.0.20230622) (4.13.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\patricia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from snscrape==0.7.0.20230622) (3.18.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\patricia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4->snscrape==0.7.0.20230622) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\patricia\\appdata\\roaming\\python\\python310\\site-packages (from beautifulsoup4->snscrape==0.7.0.20230622) (4.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\patricia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests[socks]->snscrape==0.7.0.20230622) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\patricia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests[socks]->snscrape==0.7.0.20230622) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\patricia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests[socks]->snscrape==0.7.0.20230622) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\patricia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests[socks]->snscrape==0.7.0.20230622) (2025.4.26)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\patricia\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests[socks]->snscrape==0.7.0.20230622) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install git+https://github.com/JustAnotherArchivist/snscrape.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8f4fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15acf62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: snscrape [-h] [--version] [--citation] [-v] [--dump-locals] [--retry N]\n",
      "                [-n N] [-f FORMAT | --jsonl | --jsonl-for-buggy-int-parser]\n",
      "                [--with-entity] [--since DATETIME] [--progress]\n",
      "                SCRAPER ...\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --version             show program's version number and exit\n",
      "  --citation            Display recommended citation information and exit\n",
      "                        (default: None)\n",
      "  -v, --verbose, --verbosity\n",
      "                        Increase output verbosity (default: 0)\n",
      "  --dump-locals         Dump local variables on serious log messages (warnings\n",
      "                        or higher) (default: False)\n",
      "  --retry N, --retries N\n",
      "                        When the connection fails or the server returns an\n",
      "                        unexpected response, retry up to N times with an\n",
      "                        exponential backoff (default: 3)\n",
      "  -n N, --max-results N\n",
      "                        Only return the first N results (default: None)\n",
      "  -f FORMAT, --format FORMAT\n",
      "                        Output format (default: None)\n",
      "  --jsonl               Output JSONL (default: False)\n",
      "  --jsonl-for-buggy-int-parser\n",
      "                        Output JSONL and insert extra string fields into\n",
      "                        objects for integers exceeding double precision limits\n",
      "                        (default: False)\n",
      "  --with-entity         Include the entity (e.g. user, channel) as the first\n",
      "                        output item (default: False)\n",
      "  --since DATETIME      Only return results newer than DATETIME (default:\n",
      "                        None)\n",
      "  --progress            Report progress on stderr (default: False)\n",
      "\n",
      "scrapers:\n",
      "  SCRAPER\n",
      "    facebook-community\n",
      "    facebook-group\n",
      "    facebook-user\n",
      "    instagram-hashtag\n",
      "    instagram-location\n",
      "    instagram-user\n",
      "    mastodon-profile\n",
      "    mastodon-toot\n",
      "    reddit-search\n",
      "    reddit-submission\n",
      "    reddit-subreddit\n",
      "    reddit-user\n",
      "    telegram-channel\n",
      "    twitter-cashtag\n",
      "    twitter-community\n",
      "    twitter-hashtag\n",
      "    twitter-list-posts\n",
      "    twitter-profile\n",
      "    twitter-search\n",
      "    twitter-trends\n",
      "    twitter-tweet\n",
      "    twitter-user\n",
      "    twitter-users\n",
      "    vkontakte-user\n",
      "    weibo-user\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Run the snscrape help to see what options / parameters we can use\n",
    "cmd = 'snscrape --help'\n",
    "\n",
    "#This is similar to running os.system(cmd), which would show the output of running the command in the Terminal\n",
    "#window from where I started my Jupyter Notebook (which is what I used to develop this code)\n",
    "#By using subprocees, I capture the commands's output into a variable, whose content I can then print here.\n",
    "output = subprocess.check_output(cmd, shell=True)\n",
    "                                 \n",
    "print(output.decode(\"utf-8\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d98795b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'data/raw/tweets/bitcoins_query-tweets.jsonl'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "#Using the OS library to call CLI commands in Python\n",
    "os.system(f'snscrape --max-results 100 --jsonl --progress --since 2018-12-01 twitter-search \"#bitcoins lang:fr until:2019-12-31\" > {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1cccbc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = date(2018, 1, 1)\n",
    "start = start.strftime('%Y-%m-%d')\n",
    "\n",
    "stop = date(2024, 12, 31)\n",
    "stop = stop.strftime('%Y-%m-%d')\n",
    "\n",
    "keyword = 'bitcoins'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "97d7615c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://twitter.com/search?f=live&lang=en&q=volatilit%C3%A9+bitcoin+lang%3Afr+-is%3Aretweet+min_retweets%3A0+min_faves%3A20+lang%3Afr+since%3A2018-01-01+until%3A2024-12-31&src=spelling_expansion_revert_click: SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=volatilit%C3%A9+bitcoin+lang%3Afr+-is%3Aretweet+min_retweets%3A0+min_faves%3A20+lang%3Afr+since%3A2018-01-01+until%3A2024-12-31&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\"))\n",
      "4 requests to https://twitter.com/search?f=live&lang=en&q=volatilit%C3%A9+bitcoin+lang%3Afr+-is%3Aretweet+min_retweets%3A0+min_faves%3A20+lang%3Afr+since%3A2018-01-01+until%3A2024-12-31&src=spelling_expansion_revert_click failed, giving up.\n",
      "Errors: SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=volatilit%C3%A9+bitcoin+lang%3Afr+-is%3Aretweet+min_retweets%3A0+min_faves%3A20+lang%3Afr+since%3A2018-01-01+until%3A2024-12-31&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=volatilit%C3%A9+bitcoin+lang%3Afr+-is%3Aretweet+min_retweets%3A0+min_faves%3A20+lang%3Afr+since%3A2018-01-01+until%3A2024-12-31&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=volatilit%C3%A9+bitcoin+lang%3Afr+-is%3Aretweet+min_retweets%3A0+min_faves%3A20+lang%3Afr+since%3A2018-01-01+until%3A2024-12-31&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=volatilit%C3%A9+bitcoin+lang%3Afr+-is%3Aretweet+min_retweets%3A0+min_faves%3A20+lang%3Afr+since%3A2018-01-01+until%3A2024-12-31&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\"))\n"
     ]
    },
    {
     "ename": "ScraperException",
     "evalue": "4 requests to https://twitter.com/search?f=live&lang=en&q=volatilit%C3%A9+bitcoin+lang%3Afr+-is%3Aretweet+min_retweets%3A0+min_faves%3A20+lang%3Afr+since%3A2018-01-01+until%3A2024-12-31&src=spelling_expansion_revert_click failed, giving up.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mScraperException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 24\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#We write to the JSON file by using JSONL format, which is a convenient way to store JSON objects in a line-delimited format.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#I will use the following Twitter search operators:\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# since - start date for Tweets collection \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#                 but it looks like this will exclude tweets with links from the search results\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# -filter:replies - removes @reply tweets from search results\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, tweet \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sntwitter\u001b[38;5;241m.\u001b[39mTwitterSearchScraper(search_query)\u001b[38;5;241m.\u001b[39mget_items()):\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10\u001b[39m :\n\u001b[0;32m     26\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PATRICIA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\snscrape\\modules\\twitter.py:1763\u001b[0m, in \u001b[0;36mTwitterSearchScraper.get_items\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1760\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariables\u001b[39m\u001b[38;5;124m'\u001b[39m: variables, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m: features}\n\u001b[0;32m   1761\u001b[0m paginationParams \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariables\u001b[39m\u001b[38;5;124m'\u001b[39m: paginationVariables, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m: features}\n\u001b[1;32m-> 1763\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_api_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline\u001b[39m\u001b[38;5;124m'\u001b[39m, _TwitterAPIType\u001b[38;5;241m.\u001b[39mGRAPHQL, params, paginationParams, cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cursor, instructionsPath \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_by_raw_query\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_timeline\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeline\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstructions\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m   1764\u001b[0m \t\u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graphql_timeline_instructions_to_tweets(obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_by_raw_query\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_timeline\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeline\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstructions\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\PATRICIA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\snscrape\\modules\\twitter.py:915\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._iter_api_data\u001b[1;34m(self, endpoint, apiType, params, paginationParams, cursor, direction, instructionsPath)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetrieving scroll page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcursor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 915\u001b[0m \tobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_api_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapiType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreqParams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    916\u001b[0m \t\u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m    918\u001b[0m \t\u001b[38;5;66;03m# No data format test, just a hard and loud crash if anything's wrong :-)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PATRICIA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\snscrape\\modules\\twitter.py:883\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._get_api_data\u001b[1;34m(self, endpoint, apiType, params, instructionsPath)\u001b[0m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_api_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, endpoint, apiType, params, instructionsPath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 883\u001b[0m \t\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_guest_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    884\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m apiType \u001b[38;5;129;01mis\u001b[39;00m _TwitterAPIType\u001b[38;5;241m.\u001b[39mGRAPHQL:\n\u001b[0;32m    885\u001b[0m \t\tparams \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39murlencode({k: json\u001b[38;5;241m.\u001b[39mdumps(v, separators \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems()}, quote_via \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39mquote)\n",
      "File \u001b[1;32mc:\\Users\\PATRICIA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\snscrape\\modules\\twitter.py:825\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._ensure_guest_token\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_guestTokenManager\u001b[38;5;241m.\u001b[39mtoken \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    824\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetrieving guest token\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 825\u001b[0m \tr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_baseUrl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponseOkCallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_guest_token_response\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    826\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m (match \u001b[38;5;241m:=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.cookie = decodeURIComponent\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgt=(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+); Max-Age=10800; Domain=\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.twitter\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.com; Path=/; Secure\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m);\u001b[39m\u001b[38;5;124m'\u001b[39m, r\u001b[38;5;241m.\u001b[39mtext)):\n\u001b[0;32m    827\u001b[0m \t\t_logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFound guest token in HTML\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\PATRICIA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\snscrape\\base.py:275\u001b[0m, in \u001b[0;36mScraper._get\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 275\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\PATRICIA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\snscrape\\base.py:271\u001b[0m, in \u001b[0;36mScraper._request\u001b[1;34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects, proxies)\u001b[0m\n\u001b[0;32m    269\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39mfatal(msg)\n\u001b[0;32m    270\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39mfatal(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mErrors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(errors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 271\u001b[0m \t\u001b[38;5;28;01mraise\u001b[39;00m ScraperException(msg)\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReached unreachable code\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mScraperException\u001b[0m: 4 requests to https://twitter.com/search?f=live&lang=en&q=volatilit%C3%A9+bitcoin+lang%3Afr+-is%3Aretweet+min_retweets%3A0+min_faves%3A20+lang%3Afr+since%3A2018-01-01+until%3A2024-12-31&src=spelling_expansion_revert_click failed, giving up."
     ]
    }
   ],
   "source": [
    "maxTweets = 100\n",
    "\n",
    "#We are going to write the data into a JSON file\n",
    "filename = 'data/raw/tweets/bitcoins_query-tweets.jsonl'\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "# Set up the query for snscrape\n",
    "query = 'volatilité bitcoin lang:fr -is:retweet'\n",
    "# Other parameters for filtering tweets\n",
    "min_likes = 20  # Nombre minimum de likes pour filtrer les tweets\n",
    "lang = \"fr\"  # Langue des tweets à récupérer\n",
    "since_year = 2018  # Année de début pour les tweets\n",
    "until_year = 2024  # Année de fin pour les tweets\n",
    "\n",
    "search_query = f\"{query} min_retweets:0 min_faves:{min_likes} lang:{lang} since:{since_year}-01-01 until:{until_year}-12-31\"\n",
    "#We write to the JSON file by using JSONL format, which is a convenient way to store JSON objects in a line-delimited format.\n",
    "with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "#I will use the following Twitter search operators:\n",
    "# since - start date for Tweets collection \n",
    "# stop  - stop date for Tweets collection\n",
    "# -filter:links - not very clear what this does, from Twitter search operators documentation: https://developer.twitter.com/en/docs/twitter-api/v1/rules-and-filtering/search-operators\n",
    "#                 but it looks like this will exclude tweets with links from the search results\n",
    "# -filter:replies - removes @reply tweets from search results\n",
    "    for i, tweet in enumerate(sntwitter.TwitterSearchScraper(search_query).get_items()):\n",
    "        if i > 10 :\n",
    "            break\n",
    "        tweet_data = {\n",
    "        \"Id\": tweet.id,\n",
    "        \"content\": tweet.content,\n",
    "        \"Likes\": tweet.likeCount,\n",
    "        \"Retweets\": tweet.retweetCount,\n",
    "        \"Date\": tweet.date.isoformat(),\n",
    "        \"Created_at\": tweet.date.isoformat(),\n",
    "        \"raw_content\": tweet.rawContent,\n",
    "        \"replies_count\": tweet.replyCount,\n",
    "        \"retweeted_count\": tweet.retweetCount,\n",
    "        \"likes_count\": tweet.likeCount,\n",
    "        \"hashtags\": tweet.hashtags,\n",
    "        \"cash_tags\": tweet.cashtags if tweet.cashtags else [],  # Vérifier si les cashtags existent\n",
    "        \"Language\": tweet.lang,\n",
    "        \"link\": tweet.url,\n",
    "        \"conversation_id\": tweet.conversationId,\n",
    "        \"jahr\": tweet.date.year,\n",
    "        \"monat\": tweet.date.month,\n",
    "        \"tag\": tweet.date.day,\n",
    "        \"time\": tweet.date.time().isoformat(),\n",
    "        \"user_id\": tweet.user.id,\n",
    "        \"user_name\": tweet.user.username,\n",
    "    }  \n",
    "        \n",
    "        f.write(json.dumps(tweet_data, ensure_ascii=False) + \"\\n\")  # Écrire le tweet dans le fichier JSON\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
